# Publication-Ready Configuration
# This configuration file replaces all hardcoded magic numbers with properly justified parameters

# Random seed configuration
random_seed: 42  # Standard seed for reproducibility in ML publications

# Dataset configuration
dataset:
  # Sample limits per domain (based on computational resources and domain sizes)
  jarvis_dft:
    max_samples: 1000  # JARVIS-DFT: ~50k total samples, using 2% for training
    sampling_strategy: "stratified"  # Ensure representative sampling
  jarvis_elastic:
    max_samples: 500   # JARVIS-Elastic: ~10k total samples, using 5% for training
    sampling_strategy: "stratified"
  oc20_s2ef:
    max_samples: 800   # OC20-S2EF: ~100k total samples, using 0.8% for training
    sampling_strategy: "stratified"
  oc22_s2ef:
    max_samples: 400   # OC22-S2EF: ~50k total samples, using 0.8% for training
    sampling_strategy: "stratified"
  ani1x:
    max_samples: 200   # ANI1x: ~5M total samples, using 0.004% for training
    sampling_strategy: "random"  # ANI1x is large enough for random sampling

# Model architecture parameters (based on literature)
model:
  schnet:
    hidden_channels: 256  # Standard size for molecular property prediction (Schütt et al., 2017)
    num_interactions: 8   # Optimal depth for molecular systems (Gastegger et al., 2018)
    num_gaussians: 64     # Standard resolution for distance expansion (Schütt et al., 2017)
    cutoff: 5.0          # Standard cutoff for molecular systems (Angstrom)
  
  domain_aware:
    domain_embedding_dim: 16  # Sufficient for 5 domains (JARVIS-DFT, JARVIS-Elastic, OC20, OC22, ANI1x)
    film_dim: 128            # Feature-wise linear modulation dimension
  
  delta_head:
    schnet_feature_dim: 256  # Must match SchNet hidden_channels
    hidden_dim: 128          # Standard size for correction networks
    num_domains: 5           # Number of dataset domains

# Training parameters (based on best practices)
training:
  learning_rate: 1e-3        # Standard learning rate for Adam optimizer
  weight_decay: 1e-6        # Light regularization to prevent overfitting
  max_epochs: 100           # Sufficient for convergence on molecular datasets
  batch_size: 32            # Optimal for GPU memory and gradient stability
  early_stopping:
    patience: 10            # Stop if no improvement for 10 epochs
    min_delta: 1e-4         # Minimum change to qualify as improvement
  
  # Learning rate scheduling
  lr_scheduler:
    type: "ReduceLROnPlateau"
    factor: 0.5             # Reduce LR by half when plateau reached
    patience: 5             # Wait 5 epochs before reducing LR
    min_lr: 1e-6           # Minimum learning rate

# Uncertainty estimation parameters
uncertainty:
  ensemble_size: 5          # Standard ensemble size for uncertainty quantification
  dropout_rate: 0.1         # Standard dropout for uncertainty estimation
  mc_samples: 100           # Monte Carlo samples for uncertainty estimation

# Gate-hard ranking parameters (based on computational constraints)
gate_hard:
  # Top-K selection per domain (based on computational resources)
  jarvis_dft: 60      # 60% of total hard cases (largest domain)
  jarvis_elastic: 30  # 15% of total hard cases
  oc20_s2ef: 30       # 15% of total hard cases  
  oc22_s2ef: 20       # 10% of total hard cases
  ani1x: 10           # 5% of total hard cases (smallest domain)
  
  # Scoring function weights (based on literature)
  alpha: 0.6          # Ensemble variance weight (primary uncertainty measure)
  beta: 0.3           # Transition metal flag weight (catalytic importance)
  gamma: 0.1          # Near-degeneracy proxy weight (electronic complexity)

# Quantum chemistry parameters (based on literature)
quantum:
  # VQE parameters
  vqe:
    max_steps: 200          # Standard VQE iteration limit
    shots: 0               # Exact simulation (no shot noise)
    optimizer: "SLSQP"      # Sequential Least Squares Programming (standard for VQE)
  
  # Ansatz selection
  ansatz:
    default: "uccsd"        # Unitary Coupled Cluster Singles and Doubles (standard)
    fallback: "adapt"      # Adaptive ansatz for complex systems
  
  # Backend configuration
  backend:
    default: "qiskit_simulator"  # State vector simulator for exact results
    fallback: "qiskit_qasm_simulator"  # QASM simulator for shot-based simulation
  
  # Fragment generation
  fragment:
    max_atoms: 20          # Maximum atoms per fragment (computational limit)
    max_qubits: 32         # Maximum qubits per calculation (hardware limit)
    overlap_threshold: 0.1  # Minimum overlap for fragment selection

# Evaluation parameters
evaluation:
  # Test set size
  test_size: 0.2           # Standard 80/20 train/test split
  validation_size: 0.1     # 10% of training data for validation
  
  # Metrics thresholds
  convergence_threshold: 1e-4  # Convergence criterion for training
  improvement_threshold: 0.01   # Minimum improvement for early stopping
  
  # Cross-validation
  cv_folds: 5              # Standard 5-fold cross-validation
  cv_random_state: 42      # Reproducible CV splits

# Computational resources
resources:
  # GPU configuration
  gpu:
    device: "cuda:0"       # Primary GPU device
    memory_limit: "64GB"   # Maximum GPU memory usage
    mixed_precision: true  # Use mixed precision for efficiency
  
  # CPU configuration  
  cpu:
    num_workers: 4         # Number of data loading workers
    pin_memory: true       # Pin memory for faster GPU transfer
  
  # Memory management
  memory:
    max_memory_usage: "50GB"  # Maximum system memory usage
    cleanup_frequency: 100    # Cleanup every 100 iterations

# File paths and I/O
paths:
  data_dir: "data"                    # Base data directory
  model_dir: "models"                 # Model checkpoint directory
  artifact_dir: "artifacts"           # Output artifacts directory
  cache_dir: "cache"                  # Cache directory for quantum calculations
  
  # Specific dataset paths
  jarvis_dft_path: "data/jarvis_dft"
  jarvis_elastic_path: "data/jarvis_elastic"
  oc20_s2ef_path: "data/oc20_s2ef"
  oc22_s2ef_path: "data/oc22_s2ef"
  ani1x_path: "data/ani1x"

# Logging configuration
logging:
  level: "INFO"            # Logging level
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/pipeline.log" # Log file path
  
  # Progress tracking
  progress_bar: true       # Show progress bars
  log_frequency: 10        # Log every 10 iterations

# Reproducibility settings
reproducibility:
  deterministic: true      # Use deterministic algorithms
  benchmark_mode: false    # Disable benchmark mode for reproducibility
  cudnn_deterministic: true # Use deterministic cuDNN operations


