# Multi-stage domain-aware GNN training configuration
# Optimized for progressive FiLM and LoRA training

dataset:
  # Dataset mixing strategy
  mix_strategy: "temperature"  # "uniform", "temperature", "weighted"
  temperature_tau: 2.0
  
  # Unit conversion
  unit_conversion:
    energy_unit: "eV"
    force_unit: "eV/Angstrom"
    stress_unit: "eV/Angstrom^3"
  
  # Data splits
  validation_split: 0.1
  test_split: 0.1
  random_seed: 42
  
  # Individual dataset configurations
  jarvis_dft:
    enabled: true
    max_samples: null  # Use all samples
    energy_unit: "eV"
    force_unit: "eV/Angstrom"
    stress_unit: "eV/Angstrom^3"
    temperature_range: [0, 3000]
    atomic_species: ["H", "C", "N", "O", "F", "Si", "P", "S", "Cl", "Br", "I"]
  
  jarvis_elastic:
    enabled: true
    max_samples: null
    energy_unit: "eV"
    force_unit: "eV/Angstrom"
    stress_unit: "eV/Angstrom^3"
    temperature_range: [0, 3000]
    atomic_species: ["H", "C", "N", "O", "F", "Si", "P", "S", "Cl", "Br", "I"]
  
  oc20_s2ef:
    enabled: true
    max_samples: 2000  # Increased for better domain learning
    energy_unit: "eV"
    force_unit: "eV/Angstrom"
    stress_unit: "eV/Angstrom^3"
    temperature_range: [0, 3000]
    atomic_species: ["H", "C", "N", "O", "F", "Si", "P", "S", "Cl", "Br", "I"]
  
  oc22_s2ef:
    enabled: true
    max_samples: 1000
    energy_unit: "eV"
    force_unit: "eV/Angstrom"
    stress_unit: "eV/Angstrom^3"
    temperature_range: [0, 3000]
    atomic_species: ["H", "C", "N", "O", "F", "Si", "P", "S", "Cl", "Br", "I"]
  
  ani1x:
    enabled: true
    max_samples: 2000  # Increased for better domain learning
    energy_unit: "eV"
    force_unit: "eV/Angstrom"
    stress_unit: "eV/Angstrom^3"
    temperature_range: [0, 3000]
    atomic_species: ["H", "C", "N", "O", "F"]

# Domain adapter configuration
domain_adapter:
  # Domain embedding
  domain_embedding_dim: 128  # Increased for better domain representation
  num_domains: 5  # JARVIS-DFT, JARVIS-Elastic, OC20, OC22, ANI1x
  
  # FiLM parameters
  film_dim: 256  # Increased for better modulation
  film_use_bias: true
  
  # LoRA parameters
  lora_rank: 16  # Increased for better adaptation
  lora_alpha: 32.0  # Increased scaling
  lora_dropout: 0.1
  
  # Fine-tuning parameters
  fine_tune_layers: 3  # Increased to last 3 layers
  fine_tune_lr: 1e-5
  freeze_backbone: true

# Training configuration
training:
  # Multi-task loss weights
  w_e: 1.0    # Energy weight
  w_f: 1.0    # Reduced force weight to prevent loss explosion
  w_s: 0.0    # Stress weight (typically not available)
  
  # Training parameters
  batch_size: 16
  learning_rate: 1e-5  # Further reduced learning rate for stable training
  weight_decay: 1e-6
  max_grad_norm: 1.0
  
  # Scheduler configuration
  scheduler_type: "reduce_on_plateau"
  scheduler_patience: 5  # Reduced for faster adaptation
  scheduler_factor: 0.5
  scheduler_min_lr: 1e-7
  
  # Early stopping
  early_stopping_patience: 15  # Reduced for faster convergence
  early_stopping_min_delta: 1e-6
  
  # Logging configuration
  log_every_n_steps: 50  # More frequent logging
  save_every_n_epochs: 5
  
  # Multi-task specific
  enable_per_domain_logging: true
  missing_label_strategy: "ignore"

# Model configuration
model:
  hidden_channels: 128
  num_filters: 128
  num_interactions: 6
  num_gaussians: 50
  cutoff: 10.0
  max_num_neighbors: 32
  readout: "add"
  dipole: false
  mean: null  # Will be computed from data
  std: null   # Will be computed from data
  atomref: null

# Multi-stage training configuration
multi_stage:
  # Stage 1: Foundation Training
  foundation:
    epochs: 30
    learning_rate: 1e-4
    freeze_backbone: false
    freeze_film: false
    freeze_lora: false
    freeze_domain_embeddings: false
    domain_loss_weight: 1.0
    curriculum_learning: false
  
  # Stage 2: FiLM Specialization
  film_specialization:
    epochs: 20
    learning_rate: 5e-5
    freeze_backbone: true
    freeze_film: false
    freeze_lora: true
    freeze_domain_embeddings: false
    domain_loss_weight: 2.0
    curriculum_learning: true
  
  # Stage 3: LoRA Fine-Tuning
  lora_finetuning:
    epochs: 15
    learning_rate: 1e-5
    freeze_backbone: true
    freeze_film: true
    freeze_lora: false
    freeze_domain_embeddings: false
    domain_loss_weight: 1.5
    curriculum_learning: true
  
  # Stage 4: Joint Refinement
  joint_refinement:
    epochs: 10
    learning_rate: 2e-5
    freeze_backbone: true
    freeze_film: false
    freeze_lora: false
    freeze_domain_embeddings: false
    domain_loss_weight: 1.0
    curriculum_learning: false
