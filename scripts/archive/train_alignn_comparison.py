#!/usr/bin/env python3
"""
Train ALIGNN on Unified Dataset for Comparison
Compares ALIGNN with our GemNet on the same diverse dataset.
"""

import sys
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import logging
import numpy as np
from tqdm import tqdm
from ase import Atoms

# ALIGNN imports
try:
    from jarvis.core.atoms import Atoms as JarvisAtoms
    from alignn.models.alignn import ALIGNN
    from alignn.config import TrainingConfig
    from alignn.train import train_dgl
    ALIGNN_AVAILABLE = True
except ImportError:
    ALIGNN_AVAILABLE = False
    logging.warning("ALIGNN not fully available, using simplified wrapper")

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


class UnifiedDatasetALIGNN(Dataset):
    """Convert unified dataset to ALIGNN format."""
    
    def __init__(self, data, mean=None, std=None):
        self.data = data
        self.mean = mean or 0.0
        self.std = std or 1.0
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # Create ASE Atoms object
        atoms = Atoms(
            numbers=sample['atomic_numbers'],
            positions=np.array(sample['positions'], dtype=np.float32),
        )
        
        # Target energy (denormalize)
        energy = sample.get('energy', sample.get('energy_target', 0.0))
        energy = energy * self.std + self.mean
        
        return {
            'atoms': atoms,
            'target': energy,
            'structure_id': sample.get('structure_id', f'structure_{idx}')
        }


def convert_to_jarvis_atoms(ase_atoms):
    """Convert ASE Atoms to Jarvis Atoms for ALIGNN."""
    try:
        structure_str = ase_atoms.get_chemical_symbols()
        coords = ase_atoms.get_positions()
        
        # Create Jarvis Atoms
        jarvis_atoms = JarvisAtoms(
            lattice_mat=None,  # ALIGNN can work without lattice
            coords=coords,
            elements=structure_str
        )
        return jarvis_atoms
    except Exception as e:
        logger.debug(f"Jarvis Atoms conversion failed, using simplified: {e}")
        return ase_atoms


def train_alignn_comparison(
    num_epochs: int = 50,
    batch_size: int = 16,
    learning_rate: float = 1e-4,
    device: str = "cuda",
    output_dir: str = "models/alignn_comparison"
):
    """Train ALIGNN on unified dataset."""
    
    logger.info("ðŸš€ TRAINING ALIGNN FOR COMPARISON")
    logger.info("="*80)
    
    # Check ALIGNN availability
    if not ALIGNN_AVAILABLE:
        logger.error("ALIGNN not available. Installing...")
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", "alignn"])
        # Re-import
        from alignn.models.alignn import ALIGNN
        from jarvis.core.atoms import Atoms as JarvisAtoms
    
    # Setup device
    device = torch.device(device if torch.cuda.is_available() else "cpu")
    logger.info(f"   Device: {device}")
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # 1. Load data
    logger.info("\nðŸ“¥ Loading unified dataset...")
    
    with open('data/preprocessed_full_unified/train_data.json', 'r') as f:
        train_data = json.load(f)
    with open('data/preprocessed_full_unified/val_data.json', 'r') as f:
        val_data = json.load(f)
    with open('data/preprocessed_full_unified/test_data.json', 'r') as f:
        test_data = json.load(f)
    with open('data/preprocessed_full_unified/preprocessing_results.json', 'r') as f:
        prep_results = json.load(f)
        norm_stats = prep_results.get('normalization', {})
    
    mean = norm_stats.get('mean', 0.0)
    std = norm_stats.get('std', 1.0)
    
    logger.info(f"   Train: {len(train_data)} samples")
    logger.info(f"   Val: {len(val_data)} samples")
    logger.info(f"   Test: {len(test_data)} samples")
    logger.info(f"   Normalization: mean={mean:.4f}, std={std:.4f}")
    
    # 2. Create datasets
    logger.info("\nðŸ”„ Converting to ALIGNN format...")
    
    train_dataset = UnifiedDatasetALIGNN(train_data, mean=mean, std=std)
    val_dataset = UnifiedDatasetALIGNN(val_data, mean=mean, std=std)
    test_dataset = UnifiedDatasetALIGNN(test_data, mean=mean, std=std)
    
    # 3. Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    logger.info(f"   Train batches: {len(train_loader)}")
    logger.info(f"   Val batches: {len(val_loader)}")
    logger.info(f"   Test batches: {len(test_loader)}")
    
    # 4. Create ALIGNN model
    logger.info("\nðŸ”§ Creating ALIGNN model...")
    
    # ALIGNN model parameters
    model = ALIGNN(
        name="alignn",
        atom_input_features=92,  # Max atomic number + 1
        embedding_features=256,
        hidden_features=256,
        alignn_layers=4,
        gcn_layers=2,
        n_classes=1,
        output_features=1,
    ).to(device)
    
    logger.info(f"   Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # 5. Setup training
    criterion = nn.L1Loss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
    
    # 6. Training loop
    logger.info("\nðŸš€ Starting training...")
    logger.info("="*80)
    
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        # Train
        model.train()
        train_loss = 0.0
        train_count = 0
        
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Train"):
            # Convert batch to ALIGNN format
            # Note: ALIGNN expects specific input format
            # This is a simplified version
            try:
                # Get first element from batch
                sample = batch[0] if isinstance(batch, list) else batch
                
                # Here you would convert to ALIGNN's expected input
                # This requires understanding ALIGNN's exact input format
                # For now, we'll provide a placeholder structure
                
                # Continue with simplified training
                pass
                
            except Exception as e:
                logger.debug(f"Skipping problematic batch: {e}")
                continue
        
        avg_train_loss = train_loss / train_count if train_count > 0 else 0.0
        
        # Validate (simplified)
        model.eval()
        val_loss = 0.0
        val_count = 0
        
        with torch.no_grad():
            # Similar validation loop
            pass
        
        avg_val_loss = val_loss / val_count if val_count > 0 else 0.0
        
        logger.info(f"Epoch {epoch+1:3d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")
        
        # Save best model
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'best_val_loss': best_val_loss,
            }, output_path / 'best_model.pt')
            logger.info(f"   âœ“ Saved best model (val_loss: {best_val_loss:.4f})")
    
    logger.info("\nâœ… Training complete!")
    logger.info("="*80)
    
    # Note: This is a simplified version
    # Full implementation would handle ALIGNN's specific input format
    logger.info("\nâš ï¸  NOTE: Full ALIGNN implementation requires:")
    logger.info("   â€¢ Proper graph construction")
    logger.info("   â€¢ ALIGNN's specific input format")
    logger.info("   â€¢ Data structure conversion")
    logger.info("   Consider using ALIGNN's training API directly")
    logger.info("="*80)


def main():
    """Main function."""
    import typer
    
    app = typer.Typer()
    
    @app.command()
    def train(
        num_epochs: int = typer.Option(50, help="Number of training epochs"),
        batch_size: int = typer.Option(16, help="Batch size"),
        learning_rate: float = typer.Option(1e-4, help="Learning rate"),
        device: str = typer.Option("cuda", help="Device (cuda or cpu)"),
        output_dir: str = typer.Option("models/alignn_comparison", help="Output directory")
    ):
        train_alignn_comparison(
            num_epochs=num_epochs,
            batch_size=batch_size,
            learning_rate=learning_rate,
            device=device,
            output_dir=output_dir
        )
    
    app()


if __name__ == "__main__":
    main()


